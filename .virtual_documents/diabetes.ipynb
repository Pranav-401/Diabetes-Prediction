# !pip install missingno


import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm    
import matplotlib.pyplot as plt
from sklearn.preprocessing import scale , StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import confusion_matrix , accuracy_score , mean_squared_error , r2_score , roc_auc_score , roc_curve , classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier
from sklearn.model_selection import KFold
import missingno as msno
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import RobustScaler
import warnings
warnings.simplefilter(action='ignore')
sns.set()
plt.style.use("ggplot")
%matplotlib inline


# read the database for dir
df = pd.read_csv("diabetes.csv")


df.head()


df.info()


df.columns


# independent feature -> 'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age', 
# dependent feature   -> 'Outcome'
# descriptive statistics of the dataset
df.describe()


# (rows , column)


df.shape  


# to get distribution of outcome which mean how mean preson get 0 or 1
# value_counts return unique value


df['Outcome'].value_counts()*100/len(df)


df.Outcome.value_counts()*100/len(df)


df.Outcome.value_counts()


# len(df) it return total number of rows 
# Converts the raw counts into percentages, by multiplying by 100 and dividing by the total.


df.Outcome.value_counts()*100/len(df)



# plot the historgram of the age variable 


plt.figure(figsize=(8,7))
plt.xlabel('Age',fontsize=10)
plt.ylabel('Count',fontsize=10)
df.Age.hist(edgecolor="black")


 df.Age.max()


df.Age.min()


print("Max Age : " + str( df.Age.max()))
print("Min Age : " + str( df.Age.min()))


df.columns


# Density plot 
fig,ax=plt.subplots(4,2 , figsize=(20,20))
sns.distplot(df.Pregnancies,bins=20,ax=ax[0,0],color="red")
sns.distplot(df.Glucose,bins=20,ax=ax[0,1],color="red")
sns.distplot(df.BloodPressure,bins=20,ax=ax[1,0],color="red")
sns.distplot(df.SkinThickness,bins=20,ax=ax[1,1],color="red")
sns.distplot(df.Insulin,bins=20,ax=ax[2,0],color="red")
sns.distplot(df.BMI,bins=20,ax=ax[2,1],color="red")
sns.distplot(df.DiabetesPedigreeFunction,bins=20,ax=ax[3,0],color="red")
sns.distplot(df.Age,bins=20,ax=ax[3,1],color="red")



df.columns


df.groupby("Outcome").agg({'Pregnancies':'mean'})


df.groupby("Outcome").agg({'Pregnancies':'max'})


df.groupby("Outcome").agg({'Glucose':'mean'})



df.groupby("Outcome").agg({'Glucose':'max'})


f,ax =plt.subplots(1,2,figsize=(18,8))
df['Outcome'].value_counts().plot.pie(explode=[0,0.1],autopct="%1.1f%%",ax=ax[0],shadow=True)
ax[0].set_title('target')
ax[0].set_ylabel('')
sns.countplot(x='Outcome', data=df, ax=ax[1])
ax[1].set_title('Outcome')
plt.show()



df.corr()


f,ax=plt.subplots(figsize=[20,15])
sns.heatmap(df.corr(), annot=True, fmt='.2f', ax=ax, cmap='magma')
ax.set_title("Correlation Matrix", fontsize=20)
plt.show()


# Data preprocessing part 


df.isnull().sum()


df.columns


df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age']]=df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age']].replace(0, np.NaN)


df.isnull().sum()


msno.bar(df, color="red")


def median_target(var):
    temp=df[df[var].notnull()]
    temp=temp[[var,'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()
    return temp



columns = df.columns
columns = columns.drop('Outcome')
for i in columns:
    median_target(i)
    df.loc[(df['Outcome']==0) & (df[i].isnull()), i] = median_target(i)[i][0]
    df.loc[(df['Outcome']==1) & (df[i].isnull()), i] = median_target(i)[i][1]


df.head()


df.isnull().sum()


# pair plot
p = sns.pairplot(df, hue="Outcome")


# Outlier Detection 
# IQR
# 50%
# 24.65-->25% + 50%
# 24.65-->25%
for feature in df :
    Q1 =df[feature].quantile(0.25)
    Q3 =df[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1-1.5*IQR
    upper = Q3+1.5*IQR
    if df[(df[feature]>upper)].any(axis=None):
        print(feature, "yes")
    else:
        print(feature, "No")


plt.figure(figsize=(8,7))
sns.boxplot(x=df["Insulin"],color="red")


Q1 = df.Insulin.quantile(0.25)
Q3 = df.Insulin.quantile(0.75)
IQR = Q3 -Q1
lower = Q1-1.5*IQR
upper = Q3+1.5*IQR
df.loc[df['Insulin']>upper,"Insulin"] = upper


plt.figure(figsize=(8,7))
sns.boxplot(x=df["Insulin"],color="red")


# LOF --> local outlier factor
lof = LocalOutlierFactor(n_neighbors=10)
lof.fit_predict(df)


df.head()


plt.figure(figsize=(8,7))
sns.boxplot(x=df["Pregnancies"],color="red")


df_score= lof.negative_outlier_factor_
np.sort(df_score)[0:20]


thresold = np.sort(df_score)[7]


outlier = df_score>thresold


outlier


df=df[outlier]


df


df.shape


df.head()


plt.figure(figsize=(8,7))
sns.boxplot(x=df["Pregnancies"],color="red")


# Feature Enginnering


NewBMI = pd.Series(["Underweight","Normal","Overweight","Obesity 1","Obesity 2","Obesity 3"], dtype = "category")


NewBMI


df['NewBMI'] = NewBMI
df.loc[df["BMI"]<18.5,"NewBMI"] = NewBMI[0]
df.loc[(df["BMI"]>18.5) & df["BMI"] <=24.9, "NewBMI"] = NewBMI[1]
df.loc[(df["BMI"]>24.9) & df["BMI"] <=29.9, "NewBMI"] = NewBMI[2]
df.loc[(df["BMI"]>29.9) & df["BMI"] <=34.9, "NewBMI"] = NewBMI[3]
df.loc[(df["BMI"]>34.9) & df["BMI"] <=39.9, "NewBMI"] = NewBMI[4]
df.loc[df["BMI"]>39.9,"NewBMI"] = NewBMI[5]



df.head()


# if insulin >= 16 & insulin <= 166 --> normal
def set_insuline(row):
    if row["Insulin"]>=16 and row["Insulin"]<=166:
        return "Noraml"
    else:
        return "Abnormal"


df = df.assign(NewInsulinScore=df.apply(set_insuline, axis=1))
# df["NewInsulinScore"] = df.apply(set_insuline, axis=1)


df.head()


# Some interval were determined according to the glucose variable  and these were assigned catergorical
NewGlucose = pd.Series(["Low","Normal","Overweight","Secert","High"], dtype = "category")
df['NewGlucose'] = NewGlucose
df.loc[df["Glucose"]<=70,"NewGlucose"] = NewGlucose[0]
df.loc[(df["Glucose"]>70) & (df["Glucose"] <= 99), "NewGlucose"] = NewGlucose[1]
df.loc[(df["Glucose"]>99) & (df["Glucose"] <= 126), "NewGlucose"] = NewGlucose[2]
df.loc[df["Glucose"]>126,"NewGlucose"] = NewGlucose[3]



df.head()


# One hot encoding 


df = pd.get_dummies(df, columns=["NewBMI", "NewInsulinScore", "NewGlucose"], drop_first=True)



df.head()


df[ df.select_dtypes(bool).columns ] = df.select_dtypes(bool).astype(int)



df.head()


df.columns


categorical_df = df[['NewBMI_Obesity 1',
       'NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight',
       'NewBMI_Underweight', 'NewInsulinScore_Noraml', 'NewGlucose_Low',
       'NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secert']]


categorical_df.head()


y = df['Outcome']
X = df.drop(['Outcome','NewBMI_Obesity 1',
       'NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight',
       'NewBMI_Underweight', 'NewInsulinScore_Noraml', 'NewGlucose_Low',
       'NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secert'], axis=1)


cols = X.columns
index = X.index


X.head()


transformer = RobustScaler().fit(X)
X = transformer.transform(X)
X = pd.DataFrame(X, columns = cols, index = index)


X.head()


X = pd.concat([X, categorical_df], axis=1)


X.head()


X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0) 


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# Machine Learning Algorithm 


log_reg = LogisticRegression()
log_reg.fit(X_train , y_train)


y_pred = log_reg.predict(X_test)


accuracy_score(y_train, log_reg.predict(X_train))


accuracy_score(y_test, log_reg.predict(X_test))


confusion_matrix(y_test, y_pred)


print(classification_report(y_test,y_pred))


knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print(accuracy_score(y_train, knn.predict(X_train)))
print(accuracy_score(y_test, knn.predict(X_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test,y_pred))



# SVM
svc = SVC(probability = True)
parameter = {
    "gamma":[0.0001,0.001,0.01,0.1],
    "C":[0.01,0.05,0.5,0.01,1,10,15,20]
}
grid_serach = GridSearchCV(svc,parameter)
grid_serach.fit(X_train,y_train)


# best parameter


grid_serach.best_params_


grid_serach.best_score_


svc = SVC(C=10,gamma=0.01,probability=True)
svc.fit(X_train, y_train)
y_pred = svc.predict(X_test)
print(accuracy_score(y_train, svc.predict(X_train)))
print(accuracy_score(y_test, svc.predict(X_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test,y_pred))



# Decision Tree


DT = DecisionTreeClassifier()
DT.fit(X_train, y_train)
y_pred = DT.predict(X_test)
print(accuracy_score(y_train, DT.predict(X_train)))
print(accuracy_score(y_test, DT.predict(X_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test,y_pred))


# hyper paramter tunning of dt


grid_param = {
    'criterion':['gini','entropy'],
    'max_depth':[3,5,7,10],
    'splitter':['best','radom'],
    'min_samples_leaf':[1,2,3,5,7],
    'min_samples_split':[1,2,3,5,7],
    'max_features':['auto','sqrt','log2']
}
grid_search_dt = GridSearchCV(DT , grid_param , cv=50 , n_jobs=1, verbose = 1)
grid_search_dt.fit(X_train, y_train)


grid_search_dt.best_params_


grid_search_dt.best_score_


DT = grid_search_dt.best_estimator_
y_pred = DT.predict(X_test)
print(accuracy_score(y_train, DT.predict(X_train)))
print(accuracy_score(y_test, DT.predict(X_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test,y_pred))


rand_clf = RandomForestClassifier(criterion = 'entropy', max_depth =15 , max_features = 0.75 , min_samples_leaf = 2 , min_samples_split = 3 , n_estimators = 130)
rand_clf.fit(X_train , y_train)


y_pred = rand_clf.predict(X_test)


y_pred = rand_clf.predict(X_test)
print(accuracy_score(y_train, rand_clf.predict(X_train)))
print(accuracy_score(y_test, rand_clf.predict(X_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test,y_pred))


gbc = GradientBoostingClassifier()
parameters = {
    'loss':['deviance','exponential'],
    'learning_rate':[0.001,0.1,1,10],
    'n_estimators':[100,150,180,200]
}
grid_search_gbc = GridSearchCV(gbc, parameters , cv = 10 , n_jobs = -1, verbose = 1)
grid_search_gbc.fit(X_train, y_train)


grid_search_gbc.best_params_


grid_search_gbc.best_score_


gbc = GradientBoostingClassifier(learning_rate = 0.1 , loss = 'exponential', n_estimators = 150)
gbc.fit(X_train , y_train)


gbc = grid_search_gbc.best_estimator_
y_pred = gbc.predict(X_test)
print(accuracy_score(y_train, gbc.predict(X_train)))
print(accuracy_score(y_test, gbc.predict(X_test)))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test,y_pred))





from xgboost import XGBClassifier



